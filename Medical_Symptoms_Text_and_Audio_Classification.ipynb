{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 302713,
          "sourceType": "datasetVersion",
          "datasetId": 125828
        },
        {
          "sourceId": 303211,
          "sourceType": "datasetVersion",
          "datasetId": 126137
        }
      ],
      "dockerImageVersionId": 21696,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/risherb/Medical-Symptoms-Text-and-Audio-Classification/blob/main/Medical_Symptoms_Text_and_Audio_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'medical-speech-transcription-and-intent:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F125828%2F302713%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20241004%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20241004T054421Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D271ac3c3f866a65dddc8feaed40084a31b7e4c9cd7b675c5ccf3ca7e2158ad99892c2dd3a112aa6e9c3a84a855216fe0ab44c06333c971de6870f6e79dde33e3646b51f6ea2d9186d298e455911a11e54ad33d6f0d5784711415b8f7237decde263eb21454cc8159a645fc88ab198ffba897ae7fdeaf3e33cdf2138ceaf26004f9338f585c1be3dffd7ec0357a7245218ce5204cdf38dfa16880d3a4708850e2044c749c0d70ed0042b5bba82e0105d84fed4c04f0f6caa181d7d5d4d1b73d0d3cd87bd13b5f796ae07ade6412ef9f2f07c443ed4779b2651a4b75757ae5f99c413e3f7097a81693fadb0eac59f8c2ebe28c5f398b67f207ecdaf78786821451,scispacy-pretrained-models:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F126137%2F303211%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20241004%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20241004T054421Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D24bc28ff4a784e5b305ea15093bb1909c0543278beafcf24e942d7bef503edb72997625643c65bede91b466a85ee1779c4a2f123187ace1c59f9e3a0ed576aacaa316ce51d1d50248790d50e88aeaa65c415d090f2ca404592de9e7e8533e5c594bb026d26df314755ed330276554fc91c14a00bfadb92efaf6c0ec129d03650f972213e047b01347ec0beb9172ae36a2dba1027a6debfa1dce2939f54e16dbac8098d75688cc374b6f266f8d1d06f7f37bf26fa0d96bc6743bf575cd60bd0f76ffc0d64579284116b110325b8159b0204ce12b3f181699405966e01d11c89d4fe594bd01180c6feb2d4eed98d9246c49c82c2d469179f69347e27919f39a7f2'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "RQAJSqfVDtLr"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Medical Text and Audio Classification with Fastai**"
      ],
      "metadata": {
        "_uuid": "57a086870bc52aaf6b03da01cffbaa9133ca7369",
        "id": "pxUrhDxDDtLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RnNZEY7PD2e2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "_uuid": "7c3a125da19eb5dbc755571865e76ebb057f4d52",
        "id": "ijAsrVUPDtL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# **Step 1: Import Python Packages**\n",
        "\n",
        "# Fastai, Librosa, Spacy, Scispacy, PySound, Seaborn, etc"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "bd086b8af02dc596bf765541e29940eda3366ca5",
        "trusted": true,
        "id": "6utT3FuEDtL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scispacy\n",
        "!pip install pysoundfile\n",
        "!apt-get install libav-tools -y\n",
        "!apt-get install zip\n",
        "!pip freeze > '../working/dockerimage_snapshot.txt'"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "_uuid": "55af58d2ae1e018a19db5043b9795b79181f0b6d",
        "trusted": true,
        "id": "P8Bta2_vDtL1",
        "outputId": "170a767e-ec6c-43a7-c04c-b341f80eb275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Collecting scispacy\n  Downloading https://files.pythonhosted.org/packages/b2/68/33d18f448dfddda2392ffd9f4ef349c3627a9bf91806f55e1bf91ed64e75/scispacy-0.1.0-py3-none-any.whl\nCollecting awscli (from scispacy)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/5b/ca70b0804813dda500736b0854ba15145442fa0a3ce3382d7688359fdd27/awscli-1.16.116-py2.py3-none-any.whl (1.5MB)\n\u001b[K    100% |████████████████████████████████| 1.5MB 17.3MB/s ta 0:00:01\n\u001b[?25hCollecting conllu (from scispacy)\n  Downloading https://files.pythonhosted.org/packages/ca/82/b02495f1c594cfb4af9b1eb8f404e35c1298a1448fc950b37f14c3e83317/conllu-1.2.3-py2.py3-none-any.whl\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from scispacy) (1.16.1)\nRequirement already satisfied: spacy==2.0.18 in /opt/conda/lib/python3.6/site-packages (from scispacy) (2.0.18)\nRequirement already satisfied: PyYAML<=3.13,>=3.10 in /opt/conda/lib/python3.6/site-packages (from awscli->scispacy) (3.12)\nRequirement already satisfied: docutils>=0.10 in /opt/conda/lib/python3.6/site-packages (from awscli->scispacy) (0.14)\nCollecting botocore==1.12.106 (from awscli->scispacy)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/27/ec2c22fdc556c142c1cdf37a7335156482e5298db71980567961ab299ea4/botocore-1.12.106-py2.py3-none-any.whl (5.3MB)\n\u001b[K    100% |████████████████████████████████| 5.3MB 8.0MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from awscli->scispacy) (0.2.0)\nRequirement already satisfied: colorama<=0.3.9,>=0.2.5 in /opt/conda/lib/python3.6/site-packages (from awscli->scispacy) (0.3.9)\nCollecting rsa<=3.5.0,>=3.1.2 (from awscli->scispacy)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/ae/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e/rsa-3.4.2-py2.py3-none-any.whl (46kB)\n\u001b[K    100% |████████████████████████████████| 51kB 20.8MB/s ta 0:00:01\n\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.6/site-packages (from spacy==2.0.18->scispacy) (1.0.0)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy==2.0.18->scispacy) (2.0.2)\nRequirement already satisfied: preshed<2.1.0,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from spacy==2.0.18->scispacy) (2.0.1)\nRequirement already satisfied: thinc<6.13.0,>=6.12.1 in /opt/conda/lib/python3.6/site-packages (from spacy==2.0.18->scispacy) (6.12.1)\nRequirement already satisfied: plac<1.0.0,>=0.9.6 in /opt/conda/lib/python3.6/site-packages (from spacy==2.0.18->scispacy) (0.9.6)\nRequirement already satisfied: ujson>=1.35 in /opt/conda/lib/python3.6/site-packages (from spacy==2.0.18->scispacy) (1.35)\nRequirement already satisfied: dill<0.3,>=0.2 in /opt/conda/lib/python3.6/site-packages (from spacy==2.0.18->scispacy) (0.2.9)\nRequirement already satisfied: regex==2018.01.10 in /opt/conda/lib/python3.6/site-packages (from spacy==2.0.18->scispacy) (2018.1.10)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.6/site-packages (from spacy==2.0.18->scispacy) (2.21.0)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore==1.12.106->awscli->scispacy) (2.6.0)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from botocore==1.12.106->awscli->scispacy) (0.9.3)\nRequirement already satisfied: urllib3<1.25,>=1.20; python_version >= \"3.4\" in /opt/conda/lib/python3.6/site-packages (from botocore==1.12.106->awscli->scispacy) (1.22)\nRequirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.6/site-packages (from rsa<=3.5.0,>=3.1.2->awscli->scispacy) (0.4.5)\nRequirement already satisfied: msgpack<0.6.0,>=0.5.6 in /opt/conda/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18->scispacy) (0.5.6)\nRequirement already satisfied: msgpack-numpy<0.4.4 in /opt/conda/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18->scispacy) (0.4.3.2)\nRequirement already satisfied: cytoolz<0.10,>=0.9.0 in /opt/conda/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18->scispacy) (0.9.0.1)\nRequirement already satisfied: wrapt<1.11.0,>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18->scispacy) (1.10.11)\nRequirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18->scispacy) (4.31.1)\nRequirement already satisfied: six<2.0.0,>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18->scispacy) (1.12.0)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18->scispacy) (3.0.4)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18->scispacy) (2.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18->scispacy) (2018.11.29)\nRequirement already satisfied: toolz>=0.8.0 in /opt/conda/lib/python3.6/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy==2.0.18->scispacy) (0.9.0)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastai.text import *\n",
        "from fastai.vision import *\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "import scispacy\n",
        "import librosa\n",
        "import librosa.display\n",
        "import soundfile as sf\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from collections import Counter\n",
        "import IPython\n",
        "import os\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_kg_hide-input": true,
        "_kg_hide-output": false,
        "trusted": true,
        "id": "61xWust8DtL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **Step 3: Define Helper Functions**\n",
        "\n",
        "# Create spectrograms and word frequency plots"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "aecf19b48d10c2a80ed7d4d04943c9b670363178",
        "trusted": true,
        "id": "jHfA1kOODtL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wav_info(wav_file):\n",
        "    data, rate = sf.read(wav_file)\n",
        "    return data, rate\n",
        "\n",
        "def create_spectrogram(wav_file):\n",
        "    # adapted from Andrew Ng Deep Learning Specialization Course 5\n",
        "    data, rate = get_wav_info(wav_file)\n",
        "    nfft = 200 # Length of each window segment\n",
        "    fs = 8000 # Sampling frequencies\n",
        "    noverlap = 120 # Overlap between windows\n",
        "    nchannels = data.ndim\n",
        "    if nchannels == 1:\n",
        "        pxx, freqs, bins, im = plt.specgram(data, nfft, fs, noverlap = noverlap)\n",
        "    elif nchannels == 2:\n",
        "        pxx, freqs, bins, im = plt.specgram(data[:,0], nfft, fs, noverlap = noverlap)\n",
        "    return pxx\n",
        "\n",
        "def create_melspectrogram(filename,name):\n",
        "    # adapted from https://www.kaggle.com/devilsknight/sound-classification-using-spectrogram-images\n",
        "    plt.interactive(False)\n",
        "    clip, sample_rate = librosa.load(filename, sr=None)\n",
        "    fig = plt.figure(figsize=[0.72,0.72])\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.axes.get_xaxis().set_visible(False)\n",
        "    ax.axes.get_yaxis().set_visible(False)\n",
        "    ax.set_frame_on(False)\n",
        "    S = librosa.feature.melspectrogram(y=clip, sr=sample_rate)\n",
        "    librosa.display.specshow(librosa.power_to_db(S, ref=np.max))\n",
        "    filename  = Path('/kaggle/working/spectrograms/' + name + '.jpg')\n",
        "    plt.savefig(filename, dpi=400, bbox_inches='tight',pad_inches=0)\n",
        "    plt.close()\n",
        "    fig.clf()\n",
        "    plt.close(fig)\n",
        "    plt.close('all')\n",
        "    del filename,name,clip,sample_rate,fig,ax,S\n",
        "\n",
        "def wordBarGraphFunction(df,column,title):\n",
        "    # adapted from https://www.kaggle.com/benhamner/most-common-forum-topic-words\n",
        "    topic_words = [ z.lower() for y in\n",
        "                       [ x.split() for x in df[column] if isinstance(x, str)]\n",
        "                       for z in y]\n",
        "    word_count_dict = dict(Counter(topic_words))\n",
        "    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\n",
        "    popular_words_nonstop = [w for w in popular_words if w not in stopwords.words(\"english\")]\n",
        "    plt.barh(range(50), [word_count_dict[w] for w in reversed(popular_words_nonstop[0:50])])\n",
        "    plt.yticks([x + 0.5 for x in range(50)], reversed(popular_words_nonstop[0:50]))\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def wordCloudFunction(df,column,numWords):\n",
        "    topic_words = [ z.lower() for y in\n",
        "                       [ x.split() for x in df[column] if isinstance(x, str)]\n",
        "                       for z in y]\n",
        "    word_count_dict = dict(Counter(topic_words))\n",
        "    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\n",
        "    popular_words_nonstop = [w for w in popular_words if w not in stopwords.words(\"english\")]\n",
        "    word_string=str(popular_words_nonstop)\n",
        "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
        "                          background_color='white',\n",
        "                          max_words=numWords,\n",
        "                          width=1000,height=1000,\n",
        "                         ).generate(word_string)\n",
        "    plt.clf()\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "_uuid": "90ef211283f429d80a37ea45db0b1e0127772851",
        "_kg_hide-input": true,
        "trusted": true,
        "id": "2RR5lhqXDtL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overview = pd.read_csv('../input/medical-speech-transcription-and-intent/medical speech transcription and intent/Medical Speech, Transcription, and Intent/overview-of-recordings.csv')\n",
        "overview = overview[['file_name','phrase','prompt','overall_quality_of_the_audio','speaker_id']]\n",
        "overview=overview.dropna()\n",
        "overviewAudio = overview[['file_name','prompt']]\n",
        "overviewAudio['spec_name'] = overviewAudio['file_name'].str.rstrip('.wav')\n",
        "overviewAudio = overviewAudio[['spec_name','prompt']]\n",
        "overviewText = overview[['phrase','prompt']]\n",
        "noNaNcsv = '../input/medical-speech-transcription-and-intent/medical speech transcription and intent/Medical Speech, Transcription, and Intent/overview-of-recordings.csv'\n",
        "noNaNcsv = pd.read_csv(noNaNcsv)\n",
        "noNaNcsv = noNaNcsv.dropna()\n",
        "noNaNcsv = noNaNcsv.to_csv('overview-of-recordings.csv',index=False)\n",
        "noNaNcsv"
      ],
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_kg_hide-input": true,
        "trusted": true,
        "id": "0bobITXvDtL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1 of 3: Exploratory Data Analysis and Data Visualization**\n",
        "\n",
        "\n",
        "The dataset consists of verbal descriptions of medical symptoms (.wav, audio data) paired with text transcriptions (.csv, text data) and labeled according to the category of the ailment.\n",
        "\n",
        "Here is a sample of the .csv file that accompanies the .wav audio files."
      ],
      "metadata": {
        "id": "geUUdXcRDtL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "overview[110:120]"
      ],
      "metadata": {
        "_uuid": "bc830faf4b4972acbc45183f63b5abc7e89bf998",
        "_kg_hide-input": true,
        "trusted": true,
        "id": "wAmAiHnlDtL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The categories of ailments and the quality of the audio descriptions are described below:"
      ],
      "metadata": {
        "_uuid": "1cd046574b577158450e887eadd80f22f375ac89",
        "id": "Wu_AcKh0DtL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style(\"whitegrid\")\n",
        "promptsPlot = sns.countplot(y='prompt',data=overview)\n",
        "promptsPlot\n",
        "\n",
        "qualityPlot = sns.FacetGrid(overview,aspect=2.5)\n",
        "qualityPlot.map(sns.kdeplot,'overall_quality_of_the_audio',shade= True)\n",
        "qualityPlot.set(xlim=(2.5, overview['overall_quality_of_the_audio'].max()))\n",
        "qualityPlot.set_axis_labels('overall_quality_of_the_audio', 'Proportion')\n",
        "qualityPlot"
      ],
      "metadata": {
        "_uuid": "7d3297820b22d0916799af32fa6e704f1acaa736",
        "_kg_hide-input": true,
        "trusted": true,
        "id": "VuC-0f29DtL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here we zoom in on one specific example:\n"
      ],
      "metadata": {
        "_uuid": "38dbfec4ce35d060073532a0e374a18455211eeb",
        "id": "tnHz4bynDtL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "overview[62:63]"
      ],
      "metadata": {
        "_uuid": "f985884a4638ad4f2db519535f5ca37406238ace",
        "_kg_hide-input": true,
        "trusted": true,
        "id": "6AAKH0T4DtL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_core_sci_sm = '../input/scispacy-pretrained-models/scispacy pretrained models/Scispacy Pretrained Models/en_core_sci_sm-0.1.0/en_core_sci_sm/en_core_sci_sm-0.1.0'\n",
        "nlp = spacy.load(en_core_sci_sm)\n",
        "text = overview['phrase'][62]\n",
        "doc = nlp(text)\n",
        "print(list(doc.sents))\n",
        "print(doc.ents)\n",
        "displacy.render(next(doc.sents), style='dep', jupyter=True,options = {'compact': True, 'word_spacing': 45, 'distance': 90})"
      ],
      "metadata": {
        "_uuid": "d80b33d158ee958f04b82ce5c2b4130203883904",
        "_kg_hide-input": true,
        "trusted": true,
        "id": "C0z9GADDDtL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IPython.display.Audio('../input/medical-speech-transcription-and-intent/medical speech transcription and intent/Medical Speech, Transcription, and Intent/recordings/test/1249120_20518958_23074828.wav')"
      ],
      "metadata": {
        "_uuid": "76a92eb78edad64d3a4f7002c1b75ee5583cc387",
        "_kg_hide-input": true,
        "trusted": true,
        "id": "mgpNhs6FDtL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is another example:\n"
      ],
      "metadata": {
        "_uuid": "134214fc15be86c89894db2a1d13af7c30de5ae1",
        "id": "FoLSvUB5DtL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "overview[118:119]"
      ],
      "metadata": {
        "_uuid": "fcc3c7640d4c0ab364e2a8e3644cb70c68c3d48f",
        "_kg_hide-input": true,
        "trusted": true,
        "id": "orhQBj6ADtL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_core_sci_sm = '../input/scispacy-pretrained-models/scispacy pretrained models/Scispacy Pretrained Models/en_core_sci_sm-0.1.0/en_core_sci_sm/en_core_sci_sm-0.1.0'\n",
        "nlp = spacy.load(en_core_sci_sm)\n",
        "text = overview['phrase'][118]\n",
        "doc = nlp(text)\n",
        "print(list(doc.sents))\n",
        "print(doc.ents)\n",
        "displacy.render(next(doc.sents), style='dep', jupyter=True,options = {'compact': True, 'word_spacing': 45, 'distance': 90})"
      ],
      "metadata": {
        "_uuid": "8b7d746b6ff37bebeee99ff3845ba35b64c2bb86",
        "_kg_hide-input": true,
        "trusted": true,
        "id": "PQqFGMslDtL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IPython.display.Audio('../input/medical-speech-transcription-and-intent/medical speech transcription and intent/Medical Speech, Transcription, and Intent/recordings/test/1249120_43788827_53247832.wav')"
      ],
      "metadata": {
        "_uuid": "5ae08669d65e4aa895792c62318613df7840a0d1",
        "_kg_hide-input": true,
        "trusted": true,
        "id": "4sC4VzHmDtL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the most common words that are described in the text descriptions:\n"
      ],
      "metadata": {
        "_uuid": "6ccf40d9df5ad1256d880b9fea6df224c0d1346c",
        "id": "sRJYw6anDtL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "wordCloudFunction(overview,'phrase',10000000)"
      ],
      "metadata": {
        "_uuid": "b037d5660ddd3af691f74feddb3f97d4f8269538",
        "_kg_hide-input": true,
        "trusted": true,
        "id": "VILK_OBXDtL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "wordBarGraphFunction(overview,'phrase',\"Most Common Words in Medical Text Transcripts\")"
      ],
      "metadata": {
        "_uuid": "b9593f576f5211a9f41a3e57884956fad1d13d6d",
        "_kg_hide-input": true,
        "trusted": true,
        "id": "Le14oEmqDtL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2 of 3: Classify Ailment from Text Description**\n",
        "\n",
        "Next I will use the [fastai.text_classifier_learner()](https://docs.fast.ai/text.learner.html) functions to categorize the text descriptions according to the ailment category being described."
      ],
      "metadata": {
        "_uuid": "933ab5502145b6ae5d2ce436ec8606a8aa3c5038",
        "id": "JOQ9RA6DDtMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(7)\n",
        "path = Path('../input/medical-speech-transcription-and-intent/medical speech transcription and intent/Medical Speech, Transcription, and Intent/')\n",
        "data_clas = (TextList.from_csv(path, 'overview-of-recordings.csv',\n",
        "                               cols='phrase')\n",
        "                   .random_split_by_pct(.2)\n",
        "                   .label_from_df(cols='prompt')\n",
        "                   .databunch(bs=42))\n",
        "MODEL_PATH = \"/tmp/model/\"\n",
        "learn = text_classifier_learner(data_clas,model_dir=MODEL_PATH,arch=AWD_LSTM)\n",
        "learn.fit_one_cycle(5)"
      ],
      "metadata": {
        "_uuid": "154f1eb6aa52f6ec02d934f7d5f3242612204265",
        "trusted": true,
        "id": "9rTS2fFEDtMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(5)"
      ],
      "metadata": {
        "_uuid": "60359f7e02a5ceb236c1f44ee175dab3337e2749",
        "trusted": true,
        "id": "OvbP4hGBDtMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interp = ClassificationInterpretation.from_learner(learn)\n",
        "interp.plot_confusion_matrix(figsize=(10,10), dpi=60)"
      ],
      "metadata": {
        "_uuid": "d99271c65e6633f7181cebd102b4d5a17d93197c",
        "trusted": true,
        "id": "cRtH2ZpcDtMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "_uuid": "5c27a7f68c8a92ae02a669ca477dacf2dd8935b5",
        "id": "5sara7GGDtMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 3 of 3: Classify Ailment from Audio Description**\n",
        "\n",
        "Next I will convert the .wav files into .jpg spectrograms and then again I will attempt to classify the audio descriptions according to the category of the ailment that is being described."
      ],
      "metadata": {
        "_uuid": "b2fd5cca72f10635ccaca631900a04f3d17eeab1",
        "id": "EAuHRrGADtMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A spectrogram is a visual representation of a sound. The x-axis represents time, the y-axis represents frequency, and the third dimension (intensity or color) represents the amplitutde of a specific frequency at a specific point in time.\n"
      ],
      "metadata": {
        "_uuid": "900ed4e80bf24f2138caafd81ed2b1fb2dcc3a92",
        "id": "uFvgLfNIDtMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testAudio = \"../input/medical-speech-transcription-and-intent/medical speech transcription and intent/Medical Speech, Transcription, and Intent/recordings/train/1249120_44176037_58635902.wav\"\n",
        "x = create_spectrogram(testAudio)"
      ],
      "metadata": {
        "_uuid": "faa1da3670f01d5590540714e3085b7d53ce5afa",
        "_kg_hide-input": true,
        "trusted": true,
        "id": "L4Yare5vDtMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prior work has shown that it can be advantageous to transform the spectrogram into a melspectrogram before proceeding with computer vision applications.  For more information, see: https://en.wikipedia.org/wiki/Mel-frequency_cepstrum.\n",
        "\n",
        "Here is a representative melspectrogram:"
      ],
      "metadata": {
        "_uuid": "8fec698fbf098f6dccfbc3f5d787fdb6630500be",
        "id": "A8PUcYN4DtMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"../input/medical-speech-transcription-and-intent/medical speech transcription and intent/Medical Speech, Transcription, and Intent/recordings/train/1249120_44176037_58635902.wav\"\n",
        "clip, sample_rate = librosa.load(filename, sr=None)\n",
        "fig = plt.figure(figsize=[5,5])\n",
        "S = librosa.feature.melspectrogram(y=clip, sr=sample_rate)\n",
        "librosa.display.specshow(librosa.power_to_db(S, ref=np.max))"
      ],
      "metadata": {
        "_uuid": "782e4b2fe4d629bb43ab770a5584bda852b07070",
        "_kg_hide-input": true,
        "trusted": true,
        "id": "3koBLcHdDtMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next I convert all of the .wav audio files into .jpg melspectrogram files."
      ],
      "metadata": {
        "_uuid": "73b08a90f5ef425211a446060c5953a850626451",
        "id": "9Z8AHADhDtMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /kaggle/working/spectrograms\n",
        "\n",
        "Data_dir_train=np.array(glob(\"../input/medical-speech-transcription-and-intent/medical speech transcription and intent/Medical Speech, Transcription, and Intent/recordings/train/*\"))\n",
        "Data_dir_test=np.array(glob(\"../input/medical-speech-transcription-and-intent/medical speech transcription and intent/Medical Speech, Transcription, and Intent/recordings/test/*\"))\n",
        "Data_dir_val=np.array(glob(\"../input/medical-speech-transcription-and-intent/medical speech transcription and intent/Medical Speech, Transcription, and Intent/recordings/validate/*\"))\n",
        "\n",
        "for file in tqdm(Data_dir_train):\n",
        "    filename,name = file,file.split('/')[-1].split('.')[0]\n",
        "    create_melspectrogram(filename,name)\n",
        "for file in tqdm(Data_dir_test):\n",
        "    filename,name = file,file.split('/')[-1].split('.')[0]\n",
        "    create_melspectrogram(filename,name)\n",
        "for file in tqdm(Data_dir_val):\n",
        "    filename,name = file,file.split('/')[-1].split('.')[0]\n",
        "    create_melspectrogram(filename,name)"
      ],
      "metadata": {
        "_uuid": "bb39040f2c7ea3d08eb044acd48571f705a9490a",
        "_kg_hide-input": true,
        "trusted": true,
        "id": "iSDeISIUDtMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then I use the [fastai.create_cnn()](https://docs.fast.ai/vision.learner.html) to classify the melspectrogram images according to the category of the ailment that is being described in the audio description."
      ],
      "metadata": {
        "_uuid": "894843d2a55007e2db847b8c93e8c4ca3928e6ab",
        "id": "TX_541aJDtMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = Path('/kaggle/working/')\n",
        "np.random.seed(7)\n",
        "data = ImageDataBunch.from_df(path,df=overviewAudio, folder=\"spectrograms\", valid_pct=0.2, suffix='.jpg',\n",
        "        ds_tfms=get_transforms(), size=299, num_workers=0).normalize(imagenet_stats)\n",
        "learn = create_cnn(data, models.resnet50, metrics=accuracy)\n",
        "learn.fit_one_cycle(10)"
      ],
      "metadata": {
        "_uuid": "15d97644e5e50dbadfe25d1fe83e5176388d9c35",
        "trusted": true,
        "id": "mv-7ps_wDtMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.unfreeze()\n",
        "learn.lr_find()\n",
        "learn.recorder.plot()"
      ],
      "metadata": {
        "_uuid": "5aa034a7ad40224b706dce25193a5e93bec69bba",
        "trusted": true,
        "id": "b7eVARdNDtME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.fit_one_cycle(50)"
      ],
      "metadata": {
        "_uuid": "0c11a008d5fd0b762fefbe7e340789a44b6f2886",
        "trusted": true,
        "id": "npVuqHbDDtMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interp = ClassificationInterpretation.from_learner(learn)\n",
        "interp.plot_confusion_matrix(figsize=(10,10), dpi=60)"
      ],
      "metadata": {
        "_uuid": "d5592d40b7cf53cced14a182918c0d70f108abac",
        "trusted": true,
        "id": "yhy2ldwPDtMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r spectrograms.zip /kaggle/working/spectrograms/\n",
        "!rm -rf spectrograms/*"
      ],
      "metadata": {
        "_uuid": "a127b25c1413460b8c2f9f0446a754422cc39d20",
        "_kg_hide-input": false,
        "_kg_hide-output": true,
        "trusted": true,
        "id": "ES4CBRlGDtMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the end we were able to classify the category of the ailment being described from the audio description of the symptoms and we were able to do so with an accuracy that was much better than random chance (albeit much less accurate than earlier when we performed this same classification task using the text transcriptions instead of the audio files).\n"
      ],
      "metadata": {
        "_uuid": "b4f49f4ad12b3a0222f61dc34e157b733a8f8b0d",
        "id": "K-Q-8r8aDtML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "Here we use the [fastai.text_classifier_learner()](https://docs.fast.ai/text.learner.html) functions to classify text descriptions of medical symptoms according to the category of the ailment  being described.  Likewise, we use the [fastai.create_cnn()](https://docs.fast.ai/vision.learner.html) functions to classify melspectrogram audio descriptions of medical symptoms according to the category of the ailment being described in the audio file.\n",
        "\n",
        "\n",
        "Please note that some of the labels are incorrect and some of the audio files have poor quality.  To improve the models that are produced by this kernel I would recommend cleaning the dataset in much more detail."
      ],
      "metadata": {
        "_uuid": "6b6a97efd4cd17008bbf06f691376fdf269059f8",
        "id": "IymRprwhDtML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Credit:**"
      ],
      "metadata": {
        "_uuid": "8a98497d4ebda7a366d55be294e815bfe0562553",
        "id": "uMqOu9twDtML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "_uuid": "e242cb3c8bd62d4bde820ebc85e9817deac3d754",
        "id": "x_pI-AyxDtML"
      }
    }
  ]
}